# Docker environment variables for llama-rocm-toolbox
# Copy this file to .docker.env and fill in your API keys

# Hugging Face API token for downloading models
# Get your token from: https://huggingface.co/settings/tokens
HF_TOKEN=your_huggingface_token_here

# OpenAI API key (if using OpenAI-compatible endpoints)
# OPENAI_API_KEY=your_openai_api_key_here

# Llama server configuration
# Path to the Llama model file (GGUF format)
# Models downloaded to ./models on host will be available here
# For huggingface-cli: /workspace/models/filename.gguf
# For llama-cli -hf: /workspace/models/models--org--model/snapshots/.../filename.gguf
LLAMA_ARG_MODEL=/workspace/models/your-model.gguf

# Server host (default: 0.0.0.0 for docker)
LLAMA_ARG_HOST=0.0.0.0

# Server port (default: 8080)
LLAMA_ARG_PORT=8080

# Context size for prompt processing
LLAMA_ARG_CTX_SIZE=32768

# Number of layers to offload to GPU
LLAMA_ARG_N_GPU_LAYERS=128

# API key for server authentication
LLAMA_ARG_API_KEY=your_server_api_key_here

# Number of CPU threads for generation
LLAMA_ARG_THREADS=16

# Enable embedding extraction endpoint (true/false)
LLAMA_ARG_EMBEDDING=false

# HuggingFace cache directory (used by both huggingface-cli and llama-cli -hf)
HF_HOME=/workspace/models

# Optional: Additional environment variables
# MODEL_PATH=/workspace/models
# LOG_LEVEL=info